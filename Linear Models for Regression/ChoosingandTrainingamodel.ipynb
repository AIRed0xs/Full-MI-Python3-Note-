{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23222/2588175007.py:37: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation = train_data.corr()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request \n",
    "%matplotlib inline\n",
    "from sklearn.impute import *\n",
    "from matplotlib.pyplot import axis\n",
    "from sklearn.impute import *\n",
    "from sklearn.datasets import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "data_path = 'https://raw.githubusercontent.com/nyandwi/public_datasets/master/housing.csv'\n",
    "\n",
    "def download_read_data(path):\n",
    "    \n",
    "    \"\"\"\n",
    "     Function to retrieve data from the data paths\n",
    "     And to read the data as a pandas dataframe\n",
    "  \n",
    "    To return the dataframe\n",
    "    \"\"\" \n",
    "    \n",
    "      ## Only retrieve the directory of the data\n",
    "\n",
    "    data_path =  urllib.request.urlretrieve(path)[0]\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "cal_data = download_read_data(data_path)\n",
    "train_data, test_data = train_test_split(cal_data, test_size=0.1,random_state=20)\n",
    "cal_train = train_data.copy()\n",
    "correlation = train_data.corr()\n",
    "correlation['median_house_value']\n",
    "\n",
    "training_input_data = train_data.drop('median_house_value', axis=1)\n",
    "training_labels = train_data['median_house_value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     8231\n",
       "INLAND        5896\n",
       "NEAR OCEAN    2384\n",
       "NEAR BAY      2061\n",
       "ISLAND           4\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Categorical features are features which have categorical values. An example in our dataset is ocean_proximity that has the following values\n",
    "training_input_data['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     8231\n",
       "INLAND        5896\n",
       "NEAR OCEAN    2384\n",
       "NEAR BAY      2061\n",
       "ISLAND           4\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 5 categories: <1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND\n",
    "#We will be looking 3 techniques which are simple Python Mapping, Ordinary Encoding, and One Hot Encoding\n",
    "#mapping\n",
    "#Mapping is simple. We create a dictionary of categorical values and their corresponding numerics. And after that, we map it to the categorical feature.\n",
    "cat_feats = training_input_data['ocean_proximity']\n",
    "cat_feats.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62271805, 0.13390011, 0.78431373, ..., 0.03676084, 0.04555172,\n",
       "        0.19157667],\n",
       "       [0.29513185, 0.4218916 , 0.58823529, ..., 0.05129013, 0.05558296,\n",
       "        0.18772155],\n",
       "       [0.18965517, 0.53666312, 0.54901961, ..., 0.09736372, 0.1516198 ,\n",
       "        0.3378712 ],\n",
       "       ...,\n",
       "       [0.74340771, 0.02763018, 0.58823529, ..., 0.03942163, 0.07383654,\n",
       "        0.27298934],\n",
       "       [0.61663286, 0.16578108, 0.78431373, ..., 0.04764906, 0.11609933,\n",
       "        0.36398808],\n",
       "       [0.19269777, 0.55791711, 0.88235294, ..., 0.02653783, 0.07432988,\n",
       "        0.22199004]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing numerical features\n",
    "from sklearn.preprocessing import *\n",
    "num_feats = training_input_data.drop('ocean_proximity', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "num_scaled = scaler.fit_transform(num_feats)\n",
    "num_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67858615, -0.85796668,  0.97899282, ..., -0.33416821,\n",
       "        -0.58313172, -0.31168387],\n",
       "       [-0.93598814,  0.41242353,  0.18557502, ...,  0.04124236,\n",
       "        -0.42237836, -0.34110223],\n",
       "       [-1.45585107,  0.9187045 ,  0.02689146, ...,  1.23170093,\n",
       "         1.11663747,  0.80468775],\n",
       "       ...,\n",
       "       [ 1.27342931, -1.32674535,  0.18557502, ..., -0.26541833,\n",
       "        -0.12985994,  0.30957512],\n",
       "       [ 0.64859406, -0.71733307,  0.97899282, ..., -0.05283644,\n",
       "         0.54741244,  1.00398532],\n",
       "       [-1.44085502,  1.01246024,  1.37570172, ..., -0.59831252,\n",
       "        -0.12195403, -0.07959982]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a numerical features pipeline\n",
    "from sklearn.pipeline import *\n",
    "num_feats_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "num_feats_preprocessed = num_feats_pipe.fit_transform(num_feats)\n",
    "num_feats_preprocessed\n",
    "#With only just handful of codes, we made a pipeline which can take numerical features, impute the missing values, and scale the features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One Hot Encoding\n",
    "#One hot encoding is most preferred when the categories are not in any order and that is exactly how our categorical feature is\n",
    "#This is what I mean by saying unordered categories: If you have 3 cities and encode them with numbers (1,2,3) respectively, a machine learning model may learn that city 1 is close to city 2 and to city 3\n",
    "#On the flip side, if you have the feature of ordered ranges like low, medium, and high, then numbers can be an effective way because you want to keep the sequence of these ranges.\n",
    "#In our case, the ocean proximity feature is not in any order. By using one hot, The categories will be converted into binary representation (1s or 0s), and the orginal categorical feature will be splitted into more features, equivalent to the number of categories.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot(input_data):\n",
    "  one_hot_encoder = OneHotEncoder()\n",
    "  output = one_hot_encoder.fit_transform(input_data)\n",
    "  \n",
    "  # The output of one hot encoder is a sparse matrix. \n",
    "  # It's best to convert it into numpy array \n",
    "  output = output.toarray()\n",
    "\n",
    "  return output\n",
    "cat_feats = training_input_data[['ocean_proximity']]\n",
    "cat_feats_hot = one_hot(cat_feats)\n",
    "cat_feats_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for transforming Categorical Features\n",
    "#Even if we are only encoding categorical features, let's still use a pipeline for identity. And if you had more preprocessing functions to be done on the categorical features, you can add them in the pipeline.\n",
    "cat_feats_pipe = Pipeline([\n",
    "     ('encoder', OneHotEncoder())                      \n",
    "])\n",
    "\n",
    "cat_feats_preprocessed = cat_feats_pipe.fit_transform(cat_feats)\n",
    "##Like we saw early, the output of one hot is a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67858615, -0.85796668,  0.97899282, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.93598814,  0.41242353,  0.18557502, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.45585107,  0.9187045 ,  0.02689146, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.27342931, -1.32674535,  0.18557502, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.64859406, -0.71733307,  0.97899282, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.44085502,  1.01246024,  1.37570172, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import *\n",
    "#The Transformer required lists of features\n",
    "\n",
    "num_list = list(num_feats)\n",
    "cat_list = list(cat_feats)\n",
    "final_pip = ColumnTransformer([\n",
    "    ('num', num_feats_pipe, num_list),    \n",
    "    ('cat', cat_feats_pipe, cat_list) \n",
    "])\n",
    "training_data_preprocessed = final_pip.fit_transform(training_input_data)\n",
    "training_data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use Linear Regression model which is available in sklearn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After we have created the model, we fit on the input training data and output labels.\n",
    "reg_model.fit(training_data_preprocessed, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.57624687e+04, -5.70484452e+04,  1.31317374e+04, -7.80834727e+03,\n",
       "        2.77116096e+04, -5.07070403e+04,  3.55449252e+04,  7.27323876e+04,\n",
       "        1.85839106e+17,  1.85839106e+17,  1.85839106e+17,  1.85839106e+17,\n",
       "        1.85839106e+17])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# machine learning is only 5% or so of the whole ML project, the rest percentage being for data processing\n",
    "#Great, that was fast! The model is now trained on the training set\n",
    "#let's take things little deep.\n",
    "#weights and bias? These are two paremeters of any typical ML model. It is possible to access the model paremeters, here is how.\n",
    "#Coef or coefficients are feferred to as weights\n",
    "reg_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8583910608472304e+17"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept is what can be compared to the bias\n",
    "reg_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one thing to note is that each coefficient correspond to each feature, and the biases term is added as a constant\n",
    "# These are what makes up a model, and in our case, it is a linear equation, hence a linear model\n",
    "#model = Coeff 0 Feature 0 + Coeff 1 Feature 1 + .......Coeff 13 Feature 13 + bias*\n",
    "#simplest form of a linear equation is y=ax+b. a stands for the coefficient/weight and b stands for intercept/bias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
