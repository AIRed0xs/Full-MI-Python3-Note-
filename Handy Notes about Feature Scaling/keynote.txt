http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html

https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling

Machine Learning models are very selective about the type and range of values that have to go for their input in order to work well. With the exception of decision trees, most ML models will expect you to scale the input features. What is feature scaling?

Feature scaling is nothing other than transforming the numerical features into a small range of values. In this notebook, we will see the following scaling technique:

    Normalization
    Standardizatiom
    Robust Scaling

Normalization and Standardization can be used or applied interchangeably, but they are quite different and they are suited for different purposes.


Final Notes

Scaling the input data before feeding it to a machine learning model is always a good practice.

Here are the punchlines:

    Scaling the features helps the model to converge faster.
    Normalization is scaling the data to be between 0 and 1. It is preferred when the data has not a normal distribution
    Standardization is scaling the data to have 0 mean and unit standard deviation. It is preferred when the data has a normal or gaussian distribution.
    Robust scaling technique is used if the data has many outliers.
    In most cases, the choice of scaling technique won't make much difference (or it can). Try all of them and see what work best with your data.
    Only the features are scaled. The labels should not be scaled.
    Make sure to not fit the scaler on test data. Only transfom.

Don't: scaler.fit_transfrom(X_test)
Do: scaler.transform(X_test)